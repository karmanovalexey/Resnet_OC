{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd02b21c5890ddc9f3667837a88cbba89040600cdbfeccf8fca1d283f5ab543305a",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "2b21c5890ddc9f3667837a88cbba89040600cdbfeccf8fca1d283f5ab543305a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkarmanovalexey\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.28 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.22\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcolorful-water-3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/karmanovalexey/Resnet-MOC-Training\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/karmanovalexey/Resnet-MOC-Training/runs/2dsvsl8v\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/alexey/development/Resnet_OC/wandb/run-20210429_181412-2dsvsl8v\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "Run properties: {'model': 'resnet_moc', 'height': 600, 'epochs': 12, 'bs': 2, 'pretrained': True, 'savedir': 'resnet_moc_0'}\n",
      "========== TRAINING ===========\n",
      "/home/alexey/Datasets/Mapillary/train/1920_1080/images\n",
      "Loaded 23 batches\n",
      "<All keys matched successfully>\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]1 -----\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 15905\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program failed with code 1.  Press ctrl-c to abort syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /home/alexey/development/Resnet_OC/wandb/run-20210429_181412-2dsvsl8v/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /home/alexey/development/Resnet_OC/wandb/run-20210429_181412-2dsvsl8v/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcolorful-water-3\u001b[0m: \u001b[34mhttps://wandb.ai/karmanovalexey/Resnet-MOC-Training/runs/2dsvsl8v\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 169, in <module>\n",
      "    main(parser.parse_args())\n",
      "  File \"train.py\", line 148, in main\n",
      "    train(args)\n",
      "  File \"train.py\", line 103, in train\n",
      "    outputs = model(inputs)\n",
      "  File \"/home/alexey/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/alexey/anaconda3/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 165, in forward\n",
      "    return self.module(*inputs[0], **kwargs[0])\n",
      "  File \"/home/alexey/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/alexey/development/Resnet_OC/resnet_moc/resnet_moc.py\", line 50, in forward\n",
      "    x = self.context(x)\n",
      "  File \"/home/alexey/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/alexey/anaconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\", line 119, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/alexey/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/alexey/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 399, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/home/alexey/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 395, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 2.75 GiB (GPU 0; 5.81 GiB total capacity; 2.30 GiB already allocated; 2.24 GiB free; 2.37 GiB reserved in total by PyTorch)\n"
     ]
    }
   ],
   "source": [
    "%pip install torchvision==0.9.0\n",
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "import wandb\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD, Adam, lr_scheduler\n",
    "\n",
    "from resnet_oc.resnet_oc import get_resnet34_oc\n",
    "from resnet_moc.resnet_moc import get_resnet34_moc\n",
    "from resnet_oc_lw.resnet_oc_lw import get_resnet34_oc_lw\n",
    "from resnet_ocr.resnet_ocr import get_resnet34_ocr\n",
    "from utils.mapillary import mapillary\n",
    "from utils.mapillary_pallete import MAPILLARY_LOSS_WEIGHTS\n",
    "from val import val, val_ocr\n",
    "\n",
    "NUM_CLASSES = 66\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args():\n",
    "    def __init__(self):\n",
    "        self.model = 'resnet_oc'\n",
    "        self.data_dir = '/home/jupyter/work/resources/Mapillary'\n",
    "        self.height = 600\n",
    "        self.num_epochs = 200\n",
    "        self.batch_size = 5\n",
    "        self.save_dir = 'resnet_oc_0'\n",
    "        self.pretrained = True\n",
    "        self.resume = True\n",
    "        self.wandb = True\n",
    "        self.project_name = 'Resnet-MOC-Training'\n",
    "        self.epochs_save = 3\n",
    "\n",
    "args = args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss2d(torch.nn.Module):\n",
    "    def __init__(self, model_name, weights, ocr_coeff = 0.4):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.loss = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "        self.k = ocr_coeff\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        if self.model_name == 'resnet_ocr':\n",
    "            (out_aux, out) = outputs\n",
    "            aux_loss = self.loss(out_aux, targets)\n",
    "            out_loss = self.loss(out, targets)\n",
    "            return self.k*aux_loss + out_loss\n",
    "        else:\n",
    "            return self.loss(outputs, targets)\n",
    "\n",
    "def get_model(model_name, pretrained=False):\n",
    "    if model_name == 'resnet_oc':\n",
    "        return get_resnet34_oc(pretrained)\n",
    "    elif model_name == 'resnet_oc_lw':\n",
    "        return get_resnet34_oc_lw(pretrained)\n",
    "    elif model_name == 'resnet_ocr':\n",
    "        return get_resnet34_ocr(pretrained)\n",
    "    elif model_name == 'resnet_moc':\n",
    "        return get_resnet34_moc(pretrained)\n",
    "    else:\n",
    "        raise NotImplementedError('Unknown model')\n",
    "\n",
    "def get_last_state(path):\n",
    "    list_of_files = glob.glob(path + \"/model-*.pth\")\n",
    "    max=0\n",
    "    for file in list_of_files:\n",
    "        num = int(re.search(r'model-(\\d*)', file).group(1))  \n",
    "\n",
    "        max = num if num > max else max \n",
    "    return max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    #Get training data\n",
    "    assert os.path.exists(args.data_dir), \"Error: datadir (dataset directory) could not be loaded\"\n",
    "    dataset_train = mapillary(args.data_dir, 'train', height=args.height, part=1.)\n",
    "    loader = DataLoader(dataset_train, num_workers=4, batch_size=args.batch_size, shuffle=True)\n",
    "    print('Loaded', len(loader), 'batches')\n",
    "\n",
    "    model = get_model(args.model, args.pretrained)\n",
    "    model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    loss_weights = torch.Tensor(MAPILLARY_LOSS_WEIGHTS).cuda()\n",
    "    criterion = CrossEntropyLoss2d(args.model, loss_weights)\n",
    "\n",
    "    savedir = args.save_dir\n",
    "    savedir = f'./save/{savedir}'\n",
    "\n",
    "    optimizer = Adam(model.parameters(), 5e-4, (0.9, 0.999),  eps=1e-08, weight_decay=1e-4)\n",
    "    lambda1 = lambda epoch: pow((1-((epoch-1)/args.num_epochs)),0.9)\n",
    "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
    "    \n",
    "    start_epoch = 1\n",
    "    if args.resume:\n",
    "        #Must load weights, optimizer, epoch and best value.\n",
    "        file_resume = savedir + '/model-{}.pth'.format(get_last_state(savedir))\n",
    "        \n",
    "        assert os.path.exists(file_resume), \"Error: resume option was used but checkpoint was not found in folder\"\n",
    "        checkpoint = torch.load(file_resume)\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        optimizer.load_state_dict(checkpoint['opt'])\n",
    "        print(\"=> Loaded checkpoint at epoch {})\".format(checkpoint['epoch']))\n",
    "    \n",
    "    for epoch in range(start_epoch, args.num_epochs+1):\n",
    "        print(\"----- TRAINING - EPOCH\", epoch, \"-----\")\n",
    "\n",
    "        epoch_loss = []\n",
    "        time_train = []\n",
    "\n",
    "        model.train()\n",
    "        for step, (images, labels) in enumerate(tqdm(loader)):\n",
    "            start_time = time.time()\n",
    "\n",
    "            inputs = images.cuda()\n",
    "            targets = labels.cuda()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(outputs, targets[:, 0])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss.append(loss.data.item())\n",
    "            time_train.append(time.time() - start_time)\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                average = sum(epoch_loss) / len(epoch_loss)\n",
    "                wandb.log({\"epoch\":epoch, \"loss\":average}, step=(epoch-1)*18000 + step*args.batch_size)\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "        if args.model == 'resnet_ocr': \n",
    "            print('Val', val_ocr(args, model, part=0.05))\n",
    "        else:\n",
    "            print('Val', val(args, model, part=0.05))\n",
    "        \n",
    "        if args.epochs_save > 0 and epoch > 0 and epoch % args.epochs_save == 0:\n",
    "            filename = f'{savedir}/model-{epoch}.pth'\n",
    "            torch.save({'model':model.state_dict(), 'opt':optimizer.state_dict(),'scheduler':scheduler.state_dict(), 'epoch':epoch}, filename)\n",
    "            print(f'save: {filename} (epoch: {epoch})')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedir = args.save_dir\n",
    "savedir = f'./save/{savedir}'\n",
    "if not os.path.exists(savedir):\n",
    "    os.makedirs(savedir)\n",
    "\n",
    "config = dict(model = args.model,\n",
    "                height = args.height,\n",
    "                epochs = args.num_epochs,\n",
    "                bs = args.batch_size,\n",
    "                pretrained = args.pretrained,\n",
    "                savedir = args.save_dir)\n",
    "\n",
    "log_mode = 'online' if args.wandb else 'disabled'\n",
    "with wandb.init(project=args.project_name, config=config, mode=log_mode):\n",
    "    print('Run properties:', config)\n",
    "    print(\"========== TRAINING ===========\")\n",
    "    train(args)\n",
    "    print(\"========== TRAINING FINISHED ===========\")"
   ]
  }
 ]
}